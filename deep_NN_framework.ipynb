{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "\n",
    "    x0 = X\n",
    "    caches = {}\n",
    "#    caches = []\n",
    "    # Gotta save the inputs as the first activations\n",
    "    caches['A0'] = x0\n",
    "#    print('Input size..'+str(x0.shape))\n",
    "\n",
    "    for i in range(1, int(len(parameters.keys())/2)+1):\n",
    "#        print('Max value in W'+str(i)+'....'+str(np.max(parameters['W'+str(i)].flatten())))\n",
    "        zl = np.matmul(parameters['W'+str(i)], x0) + parameters['b'+str(i)]\n",
    "        if (i == int(len(parameters.keys())/2)):\n",
    "            # Output layer Activation is sigmoid        \n",
    "            x0 = ex_activations.sigmoid(zl)\n",
    "#            print('Fwd prop Layer..'+str(i)+'...Sigmoid..'+str(parameters['W'+str(i)].shape))            \n",
    "        else:            \n",
    "#            print('Fwd prop Layer..'+str(i)+'...Relu..'+str(parameters['W'+str(i)].shape))            \n",
    "            x0 = ex_activations.relu(zl)\n",
    "\n",
    "        caches['A'+str(i)]=x0\n",
    "        caches['Z'+str(i)]=zl\n",
    "        caches['W'+str(i)]=parameters['W'+str(i)]\n",
    "        caches['b'+str(i)]=parameters['b'+str(i)]        \n",
    "\n",
    "    x0[x0 <= 0.001] = 0.001\n",
    "    x0[x0 >= 0.999] = 0.999\n",
    "\n",
    "    AL = x0     \n",
    "    \n",
    "#    print('Caches..', len(caches))\n",
    "    return AL, caches \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layers_dims):\n",
    "\n",
    "    num_layers = len(layers_dims)\n",
    "#    print(num_layers)\n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(num_layers-1):\n",
    "        parameters['W'+str(i+1)] = np.random.randn(layers_dims[i+1], layers_dims[i]) * np.sqrt(2/(layers_dims[i+1]+layers_dims[i]))\n",
    "        parameters['b'+str(i+1)] = np.zeros((layers_dims[i+1], 1))\n",
    "#        print(parameters['b'+str(i+1)].shape)\n",
    "        \n",
    "    return parameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    for i in range(1, int(len(parameters.keys())/2)+1):\n",
    "#        print('Updating Layer..'+str(i))\n",
    "        parameters['W'+str(i)] = parameters['W'+str(i)] - (learning_rate * grads['dW'+str(i)])\n",
    "        parameters['b'+str(i)] = parameters['b'+str(i)] - (learning_rate * grads['db'+str(i)])\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation (Sigmoid -> Relu -> Relu..)\n",
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    m = len(Y.flatten())\n",
    "    # Condition the shapes so that output and labels align\n",
    "    Y = np.reshape(Y, (1,m))\n",
    "    AL = np.reshape(AL, (1,m))\n",
    "    numlayers = int(np.floor(len(caches)/4))   \n",
    "\n",
    "    da_next = np.divide(1-Y, 1-AL) - (np.divide(Y, AL))\n",
    "\n",
    "    for i in range(numlayers, 0, -1):\n",
    "        # Last layer has a different activation\n",
    "        if (i == numlayers):\n",
    "            gprime_curr = ex_activations.sigmoidDeriv(caches['Z'+str(i)])\n",
    "        else:\n",
    "            gprime_curr = ex_activations.reluDerivative(caches['Z'+str(i)])\n",
    "\n",
    "        dz = np.multiply(da_next, gprime_curr)\n",
    "\n",
    "        # Activations coming into the first layer are the inputs\n",
    "        temp_al_prev = np.transpose(caches['A'+str(i-1)])\n",
    "\n",
    "        dW = np.divide(np.matmul(dz, temp_al_prev), m)\n",
    "        db = np.divide(np.sum(dz, axis=1, keepdims=True), m)\n",
    "        \n",
    "        da_prev = np.matmul(np.transpose(caches['W'+str(i)]), dz)\n",
    "        \n",
    "        da_next = da_prev\n",
    "        grads['dW'+str(i)] = dW\n",
    "        grads['db'+str(i)] = db\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation (Sigmoid -> Relu -> Relu..)\n",
    "def L_model_backward_L2_reg(AL, Y, caches, L2_lambd=200):\n",
    "\n",
    "    grads = {}\n",
    "    m = len(Y.flatten())\n",
    "    # Condition the shapes so that output and labels align\n",
    "    Y = np.reshape(Y, (1,m))\n",
    "    AL = np.reshape(AL, (1,m))\n",
    "    numlayers = int(np.floor(len(caches)/4))   \n",
    "\n",
    "    da_next = np.divide(1-Y, 1-AL) - (np.divide(Y, AL))\n",
    "\n",
    "    for i in range(numlayers, 0, -1):\n",
    "        # Last layer has a different activation\n",
    "        if (i == numlayers):\n",
    "            gprime_curr = ex_activations.sigmoidDeriv(caches['Z'+str(i)])\n",
    "        else:\n",
    "            gprime_curr = ex_activations.reluDerivative(caches['Z'+str(i)])\n",
    "\n",
    "        dz = np.multiply(da_next, gprime_curr)\n",
    "\n",
    "        # Activations coming into the first layer are the inputs\n",
    "        temp_al_prev = np.transpose(caches['A'+str(i-1)])\n",
    "\n",
    "        dW = np.divide(np.matmul(dz, temp_al_prev), m)\n",
    "        # Gradient for W here is L2 regularized\n",
    "        L2term = (L2_lambd * np.sum(caches['W'+str(i)]))/m\n",
    "        dW += L2term\n",
    "        \n",
    "        db = np.divide(np.sum(dz, axis=1, keepdims=True), m)\n",
    "        \n",
    "        da_prev = np.matmul(np.transpose(caches['W'+str(i)]), dz)\n",
    "        \n",
    "        da_next = da_prev\n",
    "        grads['dW'+str(i)] = dW\n",
    "        grads['db'+str(i)] = db\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression cost for now\n",
    "def compute_cost(AL, Y):\n",
    "    \n",
    "#    cost = np.sum(np.multiply((1-Y),np.log((1-AL))) + np.multiply((Y),np.log((AL))))\n",
    "    m = len(Y.flatten())\n",
    "    # Condition the shapes so that output and labels align\n",
    "    Y = np.reshape(Y, (1,m))\n",
    "    AL = np.reshape(AL, (1,m))\n",
    "    \n",
    "    cost = -1/m * (np.sum(np.multiply(1-Y, np.log(1-AL))) + np.sum(np.multiply(Y, np.log(AL))))\n",
    "    \n",
    "    # Also return success rate\n",
    "    x = (AL >= 0.5)\n",
    "\n",
    "    success = 1 - (np.count_nonzero(Y - x)/m)\n",
    "    \n",
    "    return cost, success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression cost for now\n",
    "def compute_L2_reg_cost(AL, Y, parameters, L2_lambd = 200):\n",
    "    \n",
    "#    cost = np.sum(np.multiply((1-Y),np.log((1-AL))) + np.multiply((Y),np.log((AL))))\n",
    "    m = len(Y.flatten())\n",
    "    # Condition the shapes so that output and labels align\n",
    "    Y = np.reshape(Y, (1,m))\n",
    "    AL = np.reshape(AL, (1,m))\n",
    "\n",
    "    # Cross entropy cost    \n",
    "    cost = -1/m * (np.sum(np.multiply(1-Y, np.log(1-AL))) + np.sum(np.multiply(Y, np.log(AL))))\n",
    "    \n",
    "    # Compute the L2 reg cost component\n",
    "    L2cost = 0\n",
    "    for i in range(1, int(len(parameters.keys())/2)+1):\n",
    "        L2cost += np.sum(np.square(parameters['W'+str(i)]))\n",
    "\n",
    "    L2cost = (L2_lambd * L2cost)/(2*m)\n",
    "    \n",
    "    # Also return success rate\n",
    "    x = (AL >= 0.5)\n",
    "\n",
    "    success = 1 - (np.count_nonzero(Y - x)/m)\n",
    "    \n",
    "    return (cost + L2cost), success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global variables are defined in the class definition\n",
    "class Deep_NN_frmwrk:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.localfolder = '/Users/nkolli/Documents/Python/Cats_vs_Dogs'\n",
    "        # Define input feature length - number of pixels in the images here\n",
    "        self.img_h = 32\n",
    "        self.img_w = 32\n",
    "        self.img_d = 1\n",
    "    \n",
    "        dim_1 = self.img_h * self.img_w * self.img_d\n",
    "        self.layers_dims = [dim_1, 25, 13, 5, 1] #  5-layer model\n",
    "    \n",
    "        ### Train data\n",
    "        self.kfolds = 10\n",
    "        self.numbatches = 20\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 300\n",
    "        self.print_cost = 1\n",
    "        self.use_saved_params = 1\n",
    "        self.L2_lambd = 0.7\n",
    "\n",
    "    def loadSavedParams(self):\n",
    "    \n",
    "        num_layers = len(self.layers_dims)\n",
    "        \n",
    "        ### Load parameters from training\n",
    "        parameters = {}\n",
    "        for i in range(num_layers-1):\n",
    "            parameters['W'+str(i+1)] = np.loadtxt(self.localfolder+'/Out_W'+str(i+1))\n",
    "            if (len(parameters['W'+str(i+1)].shape) < 2):\n",
    "                parameters['W'+str(i+1)] = np.reshape(parameters['W'+str(i+1)], (1, len(parameters['W'+str(i+1)])))\n",
    "                \n",
    "            parameters['b'+str(i+1)] = np.loadtxt(self.localfolder+'/Out_b'+str(i+1))\n",
    "            if (len(parameters['b'+str(i+1)].shape) < 1):\n",
    "                parameters['b'+str(i+1)] = np.reshape(parameters['b'+str(i+1)], (1, 1))\n",
    "            else:\n",
    "                parameters['b'+str(i+1)] = np.reshape(parameters['b'+str(i+1)], (len(parameters['b'+str(i+1)]), 1))            \n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def saveParams(self, params):\n",
    "        # Save parameters from training\n",
    "        for i in range(1,int(len(params.keys())/2)+1):\n",
    "            np.savetxt(self.localfolder+'/Out_W'+str(i), params['W'+str(i)])\n",
    "            np.savetxt(self.localfolder+'/Out_b'+str(i), params['b'+str(i)])\n",
    "                \n",
    "            \n",
    "    def showImageData(X, Y, t_size):\n",
    "        \n",
    "    #    # To view the images, make sure labeling is correct\n",
    "        imgi = int(input('Enter an index..'))\n",
    "        while (imgi != 99):\n",
    "            print('Index chosen is...', imgi)\n",
    "            print('Label is...', Y[imgi])\n",
    "            reimg = Image.fromarray(np.reshape(X[:,imgi],(t_size)))\n",
    "    #        t_size = (320,320)\n",
    "    #        reimg = reimg.resize(t_size, Image.ANTIALIAS)\n",
    "            plt.imshow(reimg)\n",
    "            imgi = int(input('Enter an index..'))\n",
    "    \n",
    "    def splitKfold(self, X, Y, j, m):\n",
    "    \n",
    "        # K-fold xval, separate into train and x_val\n",
    "        # If no kfold just use 10% as xval\n",
    "        if (self.kfolds == 1):\n",
    "            xval = int(m/10)\n",
    "            x_train = X[:,0:m-xval]\n",
    "            y_train = Y[0:m-xval]    \n",
    "            x_xval = X[:,m-xval+1:]\n",
    "            y_xval = Y[m-xval+1:]\n",
    "        else:\n",
    "            xval = int(m/self.kfolds)\n",
    "            tmp_range = range(j*xval, (j+1)*xval)\n",
    "            x_xval = X[:, tmp_range]\n",
    "            y_xval = Y[tmp_range]\n",
    "            # splits X and Y\n",
    "            if (j==0):\n",
    "                tmp_range = range((j+1)*xval+1, m)\n",
    "                x_train = X[:, tmp_range]\n",
    "                y_train = Y[tmp_range]\n",
    "            else:\n",
    "                tmp_range1 = range(0, j*xval)\n",
    "                tmp_range2 = range((j+1)*xval+1, m)\n",
    "                x_train = np.concatenate((X[:, tmp_range1], X[:, tmp_range2]), axis=1)\n",
    "                y_train = np.concatenate((Y[tmp_range1], Y[tmp_range2]), axis=0)\n",
    "                \n",
    "    #    print(x_xval.shape, y_xval.shape, x_train.shape, y_train.shape)\n",
    "        return x_xval, y_xval, x_train, y_train\n",
    "    \n",
    "    def L_layer_model(self, X, Y, parameters):#lr was 0.009\n",
    "        \"\"\"\n",
    "        Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "        \n",
    "        Arguments:\n",
    "        X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "        Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "        layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        print_cost -- if True, it prints the cost every 100 steps\n",
    "        \n",
    "        Returns:\n",
    "        parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "        \"\"\"\n",
    "    \n",
    "        # NK: keep this for now, for testing the model\n",
    "    #    np.random.seed(1)\n",
    "        costs = []                         # keep track of cost\n",
    "        train_successes = []\n",
    "        xval_successes = []\n",
    "        # Parameters initialization\n",
    "        if (len(parameters) == 0):\n",
    "            parameters = ex_init_layer_weights.initialize_parameters_deep(self.layers_dims)\n",
    "    \n",
    "        newparams = copy.deepcopy(parameters)\n",
    "        \n",
    "        # Separate ino Training and Cross validation sets\n",
    "        m = X.shape[1]\n",
    "            \n",
    "        starttime = time.time()\n",
    "        # Loop (gradient descent)\n",
    "        for i in range(0, int(self.num_epochs/self.kfolds)): \n",
    "    \n",
    "            for j in range(0, self.kfolds):\n",
    "                \n",
    "                X_xval, Y_xval, X_train, Y_train = self.splitKfold(X, Y, j, m)\n",
    "                # Batch training, numbatches is 1 if no batch separation\n",
    "                batch_size = int(X_train.shape[1]/self.numbatches)\n",
    "            \n",
    "                for k in range(0, self.numbatches):\n",
    "                    \n",
    "                    X_batch = X_train[:,(k*batch_size):((k+1)*batch_size-1)]\n",
    "                    Y_batch = Y_train[(k*batch_size):((k+1)*batch_size-1)]\n",
    "        \n",
    "                    # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.            \n",
    "                    AL, caches = ex_fwd_prop.L_model_forward(X_batch, newparams)\n",
    "                    \n",
    "                    # Compute cost\n",
    "#                    cost, train_success = ex_compute_cost.compute_cost(AL, Y_batch)\n",
    "                    # Compute cost with L2 reg\n",
    "                    cost, train_success = ex_compute_cost.compute_L2_reg_cost(AL, Y_batch, newparams, self.L2_lambd)\n",
    "                \n",
    "                    # Backward propagation\n",
    "#                    grads = ex_back_prop.L_model_backward(AL, Y_batch, caches)\n",
    "                    # Backward propagation with L2 reg\n",
    "                    grads = ex_back_prop.L_model_backward_L2_reg(AL, Y_batch, caches, self.L2_lambd)\n",
    "             \n",
    "                    # Update parameters.\n",
    "                    newparams = ex_update_parameters.update_parameters(newparams, grads, self.learning_rate)\n",
    "                    \n",
    "            # Print the cost every nth training cycle\n",
    "            if self.print_cost and i % 1 == 0:\n",
    "                endtime = time.time()\n",
    "                print (\"Cost after iteration %i: %f, Time: %f\" %(i, cost, (endtime-starttime)))\n",
    "                costs.append(cost)\n",
    "                train_successes.append(train_success)\n",
    "                # Run xval set and compute cost.\n",
    "                AL_xval, xval = ex_fwd_prop.L_model_forward(X_xval, newparams)        \n",
    "                xval_cost, xval_success = ex_compute_cost.compute_cost(AL_xval, Y_xval)\n",
    "                xval_successes.append(xval_success)\n",
    "                # plot the cost\n",
    "                plt.plot(np.squeeze(train_successes), 'b')\n",
    "                plt.plot(np.squeeze(xval_successes), 'k')            \n",
    "                plt.ylabel('Success %')\n",
    "                plt.xlabel('iterations (per tens)')\n",
    "                plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "                plt.draw()\n",
    "                plt.pause(1)\n",
    "                # Optional save parameters every nth iteration for future use\n",
    "                self.saveParams(newparams)\n",
    "    \n",
    "                \n",
    "        return parameters, newparams, train_success\n",
    "    \n",
    "    def train_data(self):\n",
    "        \n",
    "        # Load dataset \n",
    "        X_train = np.loadtxt(self.localfolder+'/X_train.dat')\n",
    "        Y_train = np.loadtxt(self.localfolder+'/Y_train.dat')\n",
    "        \n",
    "        # Number of examples\n",
    "        m = X_train.shape[1]\n",
    "        print('Data loaded..' + str(time.ctime()) + '...' + str(X_train.shape))\n",
    "        \n",
    "        # Shuffle the data\n",
    "        marr = np.arange(m-1)\n",
    "        np.random.shuffle(marr)\n",
    "        X_train = X_train[:,marr]\n",
    "        Y_train = Y_train[marr]\n",
    "        \n",
    "        X_train = X_train/255\n",
    "    #    X_train[X_train <= 0.4] = 0\n",
    "    #    X_train[X_train > 0.4] = 1\n",
    "        \n",
    "        print('Layer dims...'+str(self.layers_dims) + '...' + str(time.ctime()))\n",
    "        parameters = {}\n",
    "        if (self.use_saved_params):\n",
    "            parameters = self.loadSavedParams()\n",
    "    \n",
    "        params, newparams, train_success = self.L_layer_model(X_train, Y_train, parameters)\n",
    "        print('Optimization done..'+str(train_success))\n",
    "        \n",
    "        self.saveParams(newparams)\n",
    "    \n",
    "            \n",
    "    def test_data(self, t_size):\n",
    "        \n",
    "        # Load test data\n",
    "        X_test = np.loadtxt(self.localfolder+'/X_test.dat')\n",
    "        Y_test = np.loadtxt(self.localfolder+'/Y_test.dat')\n",
    "        \n",
    "        # Number of examples\n",
    "        m = X_test.shape[1]\n",
    "        \n",
    "        # Shuffle the data\n",
    "        marr = np.arange(m-1)\n",
    "        np.random.shuffle(marr)\n",
    "        X_test = X_test[:,marr]\n",
    "        Y_test = Y_test[marr]\n",
    "        \n",
    "        X_test = X_test/255\n",
    "    #    X_train[X_train <= 0.4] = 0\n",
    "    #    X_train[X_train > 0.4] = 1\n",
    "        \n",
    "        parameters = self.loadSavedParams()\n",
    "        \n",
    "        ### Run forward prop to get the output activations    \n",
    "        AL, caches = ex_fwd_prop.L_model_forward(X_test, parameters)\n",
    "        \n",
    "        ### Convert AL to binary calls\n",
    "        calls = (AL >= 0.5)\n",
    "        \n",
    "        ## Calculate success percentage\n",
    "        success = 1 - np.count_nonzero(Y_test - calls)/m\n",
    "    \n",
    "        # Show images from test data and the classification results    \n",
    "#        self.showImageData(X_test, calls, (t_size))    \n",
    "        \n",
    "        return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
