{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of deep_NN_framework.ipynb","version":"0.3.2","provenance":[{"file_id":"1Z04gLu3ikel3bxRvIWSqnW3piF3XNF_E","timestamp":1555293666721},{"file_id":"https://github.com/nkolli1176/notebook_test/blob/master/deep_NN_framework.ipynb","timestamp":1554693189034}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"RvFs64TNYyzn","colab_type":"code","outputId":"4ea111e5-93e3-4717-c29f-61c64518a99a","executionInfo":{"status":"ok","timestamp":1555292868734,"user_tz":420,"elapsed":30605,"user":{"displayName":"Naveen Kolli","photoUrl":"","userId":"11684655393284509657"}},"colab":{"base_uri":"https://localhost:8080/","height":189}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","#Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"Ig__ZswXMMKU","colab_type":"code","outputId":"a34180ac-789b-493e-ef5c-64639b19a763","executionInfo":{"status":"ok","timestamp":1554832423980,"user_tz":420,"elapsed":2574,"user":{"displayName":"Naveen Kolli","photoUrl":"","userId":"11684655393284509657"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"cell_type":"code","source":["!df -h"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Filesystem      Size  Used Avail Use% Mounted on\n","overlay         359G   30G  311G   9% /\n","tmpfs           6.4G     0  6.4G   0% /dev\n","tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n","tmpfs           6.4G   12K  6.4G   1% /var/colab\n","/dev/sda1       365G   32G  334G   9% /opt/bin\n","shm              10G     0   10G   0% /dev/shm\n","tmpfs           6.4G     0  6.4G   0% /sys/firmware\n","drive            15G   14G  1.5G  91% /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"0tgYicYcMv7h","colab_type":"code","outputId":"f1359cdc-f600-445c-fe3e-618b64ed3283","executionInfo":{"status":"ok","timestamp":1554832432758,"user_tz":420,"elapsed":2511,"user":{"displayName":"Naveen Kolli","photoUrl":"","userId":"11684655393284509657"}},"colab":{"base_uri":"https://localhost:8080/","height":97}},"cell_type":"code","source":["cat ../../../etc/fstab"],"execution_count":0,"outputs":[{"output_type":"stream","text":["# UNCONFIGURED FSTAB FOR BASE SYSTEM\n","tmpfs /dev/shm tmpfs defaults,size=10g 0 0\n","tmpfs /dev/shm tmpfs defaults,size=16g 0 0\n"],"name":"stdout"}]},{"metadata":{"id":"AWclEtzxMzCA","colab_type":"code","colab":{}},"cell_type":"code","source":["!echo \"tmpfs /dev/shm tmpfs defaults,size=16g 0 0\" >> ../../../etc/fstab"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4zrANTw3M7-W","colab_type":"code","outputId":"f002be1b-89ab-46e1-db07-9a90fdf12679","executionInfo":{"status":"ok","timestamp":1554832450327,"user_tz":420,"elapsed":2623,"user":{"displayName":"Naveen Kolli","photoUrl":"","userId":"11684655393284509657"}},"colab":{"base_uri":"https://localhost:8080/","height":114}},"cell_type":"code","source":["cat ../../../etc/fstab"],"execution_count":0,"outputs":[{"output_type":"stream","text":["# UNCONFIGURED FSTAB FOR BASE SYSTEM\n","tmpfs /dev/shm tmpfs defaults,size=10g 0 0\n","tmpfs /dev/shm tmpfs defaults,size=16g 0 0\n","tmpfs /dev/shm tmpfs defaults,size=16g 0 0\n"],"name":"stdout"}]},{"metadata":{"id":"AetKLI24NAmB","colab_type":"code","colab":{}},"cell_type":"code","source":["!mount -o remount /dev/shm"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g3zpuUTVNCrQ","colab_type":"code","outputId":"b2d44175-6946-4fd6-b6f7-5398915ea15a","executionInfo":{"status":"ok","timestamp":1554832460920,"user_tz":420,"elapsed":3572,"user":{"displayName":"Naveen Kolli","photoUrl":"","userId":"11684655393284509657"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"cell_type":"code","source":["!df -h"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Filesystem      Size  Used Avail Use% Mounted on\n","overlay         359G   30G  311G   9% /\n","tmpfs           6.4G     0  6.4G   0% /dev\n","tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n","tmpfs           6.4G   12K  6.4G   1% /var/colab\n","/dev/sda1       365G   32G  334G   9% /opt/bin\n","shm              10G     0   10G   0% /dev/shm\n","tmpfs           6.4G     0  6.4G   0% /sys/firmware\n","drive            15G   14G  1.5G  91% /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"geWfNI7x7z5C","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import time\n","import copy\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dyIxbPPK7z5P","colab_type":"code","colab":{}},"cell_type":"code","source":["def L_model_forward(X, parameters):\n","\n","    x0 = X\n","    caches = {}\n","#    caches = []\n","    # Gotta save the inputs as the first activations\n","    caches['A0'] = x0\n","#    print('Input size..'+str(x0.shape))\n","\n","    for i in range(1, int(len(parameters.keys())/2)+1):\n","#        print('Max value in W'+str(i)+'....'+str(np.max(parameters['W'+str(i)].flatten())))\n","        zl = np.matmul(parameters['W'+str(i)], x0) + parameters['b'+str(i)]\n","        if (i == int(len(parameters.keys())/2)):\n","            # Output layer Activation is sigmoid        \n","            x0 = sigmoid(zl)\n","#            print('Fwd prop Layer..'+str(i)+'...Sigmoid..'+str(parameters['W'+str(i)].shape))            \n","        else:            \n","#            print('Fwd prop Layer..'+str(i)+'...Relu..'+str(parameters['W'+str(i)].shape))            \n","            x0 = relu(zl)\n","\n","        caches['A'+str(i)]=x0\n","        caches['Z'+str(i)]=zl\n","        caches['W'+str(i)]=parameters['W'+str(i)]\n","        caches['b'+str(i)]=parameters['b'+str(i)]        \n","\n","    x0[x0 <= 0.001] = 0.001\n","    x0[x0 >= 0.999] = 0.999\n","\n","    AL = x0     \n","    \n","#    print('Caches..', len(caches))\n","    return AL, caches \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zLjEtV2U7z5X","colab_type":"code","colab":{}},"cell_type":"code","source":["def initialize_parameters_deep(layers_dims):\n","\n","    num_layers = len(layers_dims)\n","#    print(num_layers)\n","    parameters = {}\n","    \n","    for i in range(num_layers-1):\n","        parameters['W'+str(i+1)] = np.random.randn(layers_dims[i+1], layers_dims[i]) * np.sqrt(2/(layers_dims[i+1]+layers_dims[i]))\n","        parameters['b'+str(i+1)] = np.zeros((layers_dims[i+1], 1))\n","#        print(parameters['b'+str(i+1)].shape)\n","        \n","    return parameters \n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mj9uaFO77z5b","colab_type":"code","colab":{}},"cell_type":"code","source":["def update_parameters(parameters, grads, learning_rate):\n","\n","    for i in range(1, int(len(parameters.keys())/2)+1):\n","#        print('Updating Layer..'+str(i))\n","        parameters['W'+str(i)] = parameters['W'+str(i)] - (learning_rate * grads['dW'+str(i)])\n","        parameters['b'+str(i)] = parameters['b'+str(i)] - (learning_rate * grads['db'+str(i)])\n","\n","    return parameters\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q92DSl967z5h","colab_type":"code","colab":{}},"cell_type":"code","source":["# Backward propagation (Sigmoid -> Relu -> Relu..)\n","def L_model_backward(AL, Y, caches):\n","\n","    grads = {}\n","    m = len(Y.flatten())\n","    # Condition the shapes so that output and labels align\n","    Y = np.reshape(Y, (1,m))\n","    AL = np.reshape(AL, (1,m))\n","    numlayers = int(np.floor(len(caches)/4))   \n","\n","    da_next = np.divide(1-Y, 1-AL) - (np.divide(Y, AL))\n","\n","    for i in range(numlayers, 0, -1):\n","        # Last layer has a different activation\n","        if (i == numlayers):\n","            gprime_curr = sigmoidDeriv(caches['Z'+str(i)])\n","        else:\n","            gprime_curr = reluDerivative(caches['Z'+str(i)])\n","\n","        dz = np.multiply(da_next, gprime_curr)\n","\n","        # Activations coming into the first layer are the inputs\n","        temp_al_prev = np.transpose(caches['A'+str(i-1)])\n","\n","        dW = np.divide(np.matmul(dz, temp_al_prev), m)\n","        db = np.divide(np.sum(dz, axis=1, keepdims=True), m)\n","        \n","        da_prev = np.matmul(np.transpose(caches['W'+str(i)]), dz)\n","        \n","        da_next = da_prev\n","        grads['dW'+str(i)] = dW\n","        grads['db'+str(i)] = db\n","\n","    return grads\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fLHws18M7z5p","colab_type":"code","colab":{}},"cell_type":"code","source":["# Backward propagation (Sigmoid -> Relu -> Relu..)\n","def L_model_backward_L2_reg(AL, Y, caches, L2_lambd=200):\n","\n","    grads = {}\n","    m = len(Y.flatten())\n","    # Condition the shapes so that output and labels align\n","    Y = np.reshape(Y, (1,m))\n","    AL = np.reshape(AL, (1,m))\n","    numlayers = int(np.floor(len(caches)/4))   \n","\n","    da_next = np.divide(1-Y, 1-AL) - (np.divide(Y, AL))\n","\n","    for i in range(numlayers, 0, -1):\n","        # Last layer has a different activation\n","        if (i == numlayers):\n","            gprime_curr = sigmoidDeriv(caches['Z'+str(i)])\n","        else:\n","            gprime_curr = reluDerivative(caches['Z'+str(i)])\n","\n","        dz = np.multiply(da_next, gprime_curr)\n","\n","        # Activations coming into the first layer are the inputs\n","        temp_al_prev = np.transpose(caches['A'+str(i-1)])\n","\n","        dW = np.divide(np.matmul(dz, temp_al_prev), m)\n","        # Gradient for W here is L2 regularized\n","        L2term = (L2_lambd * caches['W'+str(i)])/m\n","        dW += L2term\n","        \n","        db = np.divide(np.sum(dz, axis=1, keepdims=True), m)\n","        \n","        da_prev = np.matmul(np.transpose(caches['W'+str(i)]), dz)\n","        \n","        da_next = da_prev\n","        grads['dW'+str(i)] = dW\n","        grads['db'+str(i)] = db\n","\n","    return grads\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7dq3TfQZyt3z","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def sigmoid(z):\n","    sig = 1/(1 + np.exp(-z))\n","    return sig\n","\n","def relu(a):\n","    rel = np.maximum(a, 0)\n","    return rel\n","\n","def reluDerivative(x):\n","    x[x<=0] = 0\n","    x[x>0] = 1\n","    return x\n","\n","def sigmoidDeriv(z):\n","    sd = sigmoid(z)*(1 - sigmoid(z))\n","    return sd\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cxnv0nTE7z5t","colab_type":"code","colab":{}},"cell_type":"code","source":["# Logistic regression cost for now\n","def compute_cost(AL, Y):\n","    \n","#    cost = np.sum(np.multiply((1-Y),np.log((1-AL))) + np.multiply((Y),np.log((AL))))\n","    m = len(Y.flatten())\n","    # Condition the shapes so that output and labels align\n","    Y = np.reshape(Y, (1,m))\n","    AL = np.reshape(AL, (1,m))\n","    \n","    cost = -1/m * (np.sum(np.multiply(1-Y, np.log(1-AL))) + np.sum(np.multiply(Y, np.log(AL))))\n","    \n","    # Also return success rate\n","    x = (AL >= 0.5)\n","\n","    success = 1 - (np.count_nonzero(Y - x)/m)\n","    \n","    return cost, success\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ddO4gKU57z5w","colab_type":"code","colab":{}},"cell_type":"code","source":["# Logistic regression cost for now\n","def compute_L2_reg_cost(AL, Y, parameters, L2_lambd = 200):\n","    \n","#    cost = np.sum(np.multiply((1-Y),np.log((1-AL))) + np.multiply((Y),np.log((AL))))\n","    m = Y.size\n","    # Condition the shapes so that output and labels align\n","    Y = np.reshape(Y, (1,m))\n","    AL = np.reshape(AL, (1,m))\n","\n","    # Cross entropy cost    \n","    cost = -1/m * (np.sum(np.multiply(1-Y, np.log(1-AL))) + np.sum(np.multiply(Y, np.log(AL))))\n","    \n","    # Compute the L2 reg cost component\n","    L2cost = 0\n","    for i in range(1, int(len(parameters.keys())/2)+1):\n","      L2cost += np.sum(np.square(parameters['W'+str(i)]))\n","\n","    L2cost = (L2_lambd * L2cost)/(2*m)\n","    \n","    # Also return success rate\n","    x = (AL >= 0.5)\n","\n","    success = 1 - (np.count_nonzero(Y - x)/m)\n","    \n","    return (cost + L2cost), success\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wbwCWG9q7z5z","colab_type":"code","colab":{}},"cell_type":"code","source":["## Global variables are defined in the class definition\n","class Deep_NN_frmwrk:\n","    \n","    def __init__(self, name):\n","        self.name = name\n","        # self.localfolder = '/Users/administrator/Documents/Python/dim_128_mono';\n","        self.localfolder = 'gdrive/My Drive/';\n","        self.train_data_file = 'X_K_mono_train_P3.dat'\n","        self.train_labels_file = 'Y_K_mono_train_P3.dat'\n","        self.test_data_file = 'X_K_mono_test.dat'\n","        self.test_labels_file = 'Y_K_mono_test.dat'\n","\n","        \n","        # Define input feature length - number of pixels in the images here\n","        self.img_h = 128\n","        self.img_w = 128\n","        self.img_d = 1\n","    \n","        dim_1 = self.img_h * self.img_w * self.img_d\n","        self.layers_dims = [dim_1, 25, 13, 5, 1] #  5-layer model\n","    \n","        ### Train data\n","        self.numtrainfiles = 3\n","        self.kfolds = 20\n","        self.numbatches = 15 \n","        self.learning_rate = 0.005\n","        self.epochs = 100 # per data set\n","        self.print_cost = 1\n","        self.use_saved_params = 0\n","        self.L2_lambd = 0.8\n","        \n","        # To hold runtime stuff\n","        self.costs = []                         # keep track of cost\n","        self.train_successes = []\n","        self.xval_successes = []\n","\n","        # keep track of status\n","        self.starttime = time.time()\n","        self.curr_epoch = 0\n","\n","    def loadSavedParams(self):\n","    \n","        num_layers = len(self.layers_dims)\n","        \n","        ### Load parameters from training +'.dat'\n","        parameters = {}\n","        for i in range(num_layers-1):\n","            parameters['W'+str(i+1)] = np.loadtxt(self.localfolder+'Out_W'+str(i+1)+'.dat')\n","            if (len(parameters['W'+str(i+1)].shape) < 2):\n","                parameters['W'+str(i+1)] = np.reshape(parameters['W'+str(i+1)], (1, len(parameters['W'+str(i+1)])))\n","                \n","            parameters['b'+str(i+1)] = np.loadtxt(self.localfolder+'Out_b'+str(i+1)+'.dat')\n","            if (len(parameters['b'+str(i+1)].shape) < 1):\n","                parameters['b'+str(i+1)] = np.reshape(parameters['b'+str(i+1)], (1, 1))\n","            else:\n","                parameters['b'+str(i+1)] = np.reshape(parameters['b'+str(i+1)], (len(parameters['b'+str(i+1)]), 1))            \n","        \n","        return parameters\n","    \n","    def saveParams(self, params):\n","        # Save parameters from training\n","        for i in range(1,int(len(params.keys())/2)+1):\n","            np.savetxt(self.localfolder+'Out_W'+str(i)+'.dat', params['W'+str(i)])\n","            np.savetxt(self.localfolder+'Out_b'+str(i)+'.dat', params['b'+str(i)])\n","                \n","            \n","    def showImageData(X, Y, t_size):\n","        \n","    #    # To view the images, make sure labeling is correct\n","        imgi = int(input('Enter an index..'))\n","        while (imgi != 99):\n","            print('Index chosen is...', imgi)\n","            print('Label is...', Y[imgi])\n","            reimg = Image.fromarray(np.reshape(X[:,imgi],(t_size)))\n","    #        t_size = (320,320)\n","    #        reimg = reimg.resize(t_size, Image.ANTIALIAS)\n","            plt.imshow(reimg)\n","            imgi = int(input('Enter an index..'))\n","    \n","    def splitKfold(self, X, Y, j, m):\n","    \n","        # K-fold xval, separate into train and x_val\n","        # If no kfold just use 10% as xval\n","        if (self.kfolds == 1):\n","            xval = int(m/10)\n","            x_train = X[:,0:m-xval]\n","            y_train = Y[0:m-xval]    \n","            x_xval = X[:,m-xval+1:]\n","            y_xval = Y[m-xval+1:]\n","        else:\n","            xval = int(m/self.kfolds)\n","            tmp_range = range(j*xval, (j+1)*xval)\n","            x_xval = X[:, tmp_range]\n","            y_xval = Y[tmp_range]\n","            # splits X and Y\n","            if (j==0):\n","                tmp_range = range((j+1)*xval+1, m)\n","                x_train = X[:, tmp_range]\n","                y_train = Y[tmp_range]\n","            else:\n","                tmp_range1 = range(0, j*xval)\n","                tmp_range2 = range((j+1)*xval+1, m)\n","                x_train = np.concatenate((X[:, tmp_range1], X[:, tmp_range2]), axis=1)\n","                y_train = np.concatenate((Y[tmp_range1], Y[tmp_range2]), axis=0)\n","                \n","    #    print(x_xval.shape, y_xval.shape, x_train.shape, y_train.shape)\n","        return x_xval, y_xval, x_train, y_train\n","    \n","    def L_layer_model_Kfold(self, X, Y, parameters):#lr was 0.009\n","    #    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n","        \n","        # Parameters initialization\n","        if (len(parameters) == 0):\n","            parameters = initialize_parameters_deep(self.layers_dims)\n","            \n","        newparams = copy.deepcopy(parameters)\n","\n","        m = X.shape[1]\n","            \n","        # Loop (gradient descent)\n","        for i in range(0, self.kfolds): \n","    \n","            X_xval, Y_xval, X_train, Y_train = self.splitKfold(X, Y, i, m)\n","\n","            # Batch training, numbatches is 1 if no batch separation\n","            batch_size = int(X_train.shape[1]/self.numbatches)\n","\n","            for k in range(0, self.numbatches):\n","\n","                X_batch = X_train[:,(k*batch_size):((k+1)*batch_size-1)]\n","                Y_batch = Y_train[(k*batch_size):((k+1)*batch_size-1)]\n","\n","                # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.            \n","                AL, caches = L_model_forward(X_batch, newparams)\n","\n","                # Compute cost\n","#                cost, train_success = compute_cost(AL, Y_batch)\n","                # Compute cost with L2 reg\n","                cost, train_success = compute_L2_reg_cost(AL, Y_batch, newparams, self.L2_lambd)\n","\n","                # Backward propagation\n","#                grads = L_model_backward(AL, Y_batch, caches)\n","                # Backward propagation with L2 reg\n","                grads = L_model_backward_L2_reg(AL, Y_batch, caches, self.L2_lambd)\n","\n","                # Update parameters.\n","                newparams = update_parameters(newparams, grads, self.learning_rate)\n","                    \n","            # Print the cost every nth training cycle in epoch\n","            if self.print_cost and i % 8 == 0:\n","                endtime = time.time()\n","                print (\"Cost after run %i, iteration %i: %f, Time: %f\" %(self.curr_epoch, i, cost, (endtime-self.starttime)))\n","                self.costs.append(cost)\n","                self.train_successes.append(train_success)\n","\n","                # Run xval set and compute cost.\n","                AL_xval, xval = L_model_forward(X_xval, newparams)        \n","#                xval_cost, xval_success = compute_cost(AL_xval, Y_xval)\n","                xval_cost, xval_success = compute_L2_reg_cost(AL_xval, Y_xval, newparams, self.L2_lambd)\n","                self.xval_successes.append(xval_success)\n","                \n","                # plot the costs\n","                plt.plot(np.squeeze(self.train_successes), 'b')\n","                plt.plot(np.squeeze(self.xval_successes), 'k')            \n","                plt.ylabel('Success %')\n","                plt.xlabel('iterations (per tens)')\n","                plt.title(\"Learning rate =\" + str(self.learning_rate))\n","                plt.draw()\n","                plt.pause(1)\n","                \n","                # Optional save parameters every nth iteration for future use\n","                self.saveParams(newparams)\n","    \n","                \n","        return parameters, newparams, train_success\n","\n","    def L_layer_model(self, X, Y, newparams):#lr was 0.009\n","    #    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n","        \n","        # Parameters initialization\n","        if (len(newparams) == 0):\n","            newparams = initialize_parameters_deep(self.layers_dims)\n","                        \n","        # Batch training, numbatches is 1 if no batch separation\n","        batch_size = int(X.shape[1]/self.numbatches)\n","\n","        for k in range(0, self.numbatches):\n","\n","            X_batch = X[:,(k*batch_size):((k+1)*batch_size-1)]\n","            Y_batch = Y[(k*batch_size):((k+1)*batch_size-1)]\n","\n","            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.            \n","            AL, caches = L_model_forward(X_batch, newparams)\n","\n","            # Compute cost\n","#                cost, train_success = compute_cost(AL, Y_batch)\n","            # Compute cost with L2 reg\n","            cost, train_success = compute_L2_reg_cost(AL, Y_batch, newparams, self.L2_lambd)\n","\n","            # Backward propagation\n","#                grads = L_model_backward(AL, Y_batch, caches)\n","            # Backward propagation with L2 reg\n","            grads = L_model_backward_L2_reg(AL, Y_batch, caches, self.L2_lambd)\n","\n","            # Update parameters.\n","            newparams = update_parameters(newparams, grads, self.learning_rate)\n","\n","            # Print the cost every nth training cycle in epoch\n","            if self.print_cost and k % 10 == 0:\n","                endtime = time.time()\n","                print (\"Cost after run %i, iteration %i: %f, Time: %f\" %(self.curr_epoch, k, cost, (endtime-self.starttime)))\n","                self.costs.append(cost)\n","                self.train_successes.append(train_success)\n","\n","                # plot the costs\n","                plt.plot(np.squeeze(self.train_successes), 'b')\n","                plt.ylabel('Success %')\n","                plt.xlabel('iterations (per tens)')\n","                plt.title(\"Learning rate =\" + str(self.learning_rate))\n","                plt.draw()\n","                plt.pause(1)\n","\n","                # Optional save parameters every nth iteration for future use\n","                self.saveParams(newparams)\n","    \n","                \n","        return newparams, newparams, train_success\n","          \n","    def train_data(self):\n","        \n","        print('Layer dims...'+str(self.layers_dims) + '...' + str(time.ctime()))\n","\n","        parameters = {}\n","        if (self.use_saved_params):\n","            parameters = self.loadSavedParams()\n"," \n","        for k in range(self.epochs):\n","          \n","          self.curr_epoch = k\n","     \n","          for i in range(self.numtrainfiles):\n","\n","              if (self.numtrainfiles) > 1:\n","                  X_filename = str.split(self.train_data_file,'.')[0] + '_' + str(i+1) + '.' + str.split(self.train_data_file,'.')[1]\n","                  Y_filename = str.split(self.train_labels_file,'.')[0] + '_' + str(i+1) + '.' + str.split(self.train_labels_file,'.')[1]\n","              else:\n","                  X_filename = self.train_data_file\n","                  Y_filename = self.train_labels_file \n","\n","              # Load dataset \n","              X_train = np.loadtxt(self.localfolder + X_filename)\n","              Y_train = np.loadtxt(self.localfolder + Y_filename)\n","\n","              # Number of examples\n","              m = X_train.shape[1]\n","              print('Data loaded..' + str(time.ctime()) + '...' + str(X_train.shape))\n","\n","              # Shuffle the data\n","              marr = np.arange(m)\n","              np.random.shuffle(marr)\n","              X_train = X_train[:,marr]\n","              Y_train = Y_train[marr]\n","\n","              X_train = X_train/255       \n","              \n","              # After the first run, start using the derived params\n","              if k > 0 or i > 0:\n","                  parameters = newparams\n","\n","              params, newparams, train_success = self.L_layer_model(X_train, Y_train, parameters)\n","\n","        print('Optimization done..'+str(train_success))        \n","        self.saveParams(newparams)\n","    \n","            \n","    def test_data(self, t_size):\n","        \n","        # Load test data\n","        X_test = np.loadtxt(self.localfolder + self.test_data_file)\n","        Y_test = np.loadtxt(self.localfolder + self.test_labels_file)\n","        \n","        # Number of examples\n","        m = X_test.shape[1]\n","        \n","        # Shuffle the data\n","        marr = np.arange(m)\n","        np.random.shuffle(marr)\n","        X_test = X_test[:,marr]\n","        Y_test = Y_test[marr]\n","        \n","        X_test = X_test/255\n","        \n","        parameters = self.loadSavedParams()\n","        \n","        ### Run forward prop to get the output activations    \n","        AL, caches = L_model_forward(X_test, parameters)\n","        \n","        ### Convert AL to binary calls\n","        calls = (AL >= 0.5)\n","        \n","        ## Calculate success percentage\n","        success = 1 - np.count_nonzero(Y_test - calls)/m\n","    \n","        # Show images from test data and the classification results    \n","#        self.showImageData(X_test, calls, (t_size))    \n","        \n","        return success\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AXr1G1_l7z53","colab_type":"code","outputId":"8a7f5ee4-9a82-4e44-8378-e99d4c6d8010","executionInfo":{"status":"error","timestamp":1555293606126,"user_tz":420,"elapsed":134365,"user":{"displayName":"Naveen Kolli","photoUrl":"","userId":"11684655393284509657"}},"colab":{"base_uri":"https://localhost:8080/","height":592}},"cell_type":"code","source":["myNN = Deep_NN_frmwrk(\"catsDogs\")\n","print(myNN.kfolds)\n","print(myNN.localfolder)\n","\n","starttime = time.time()\n","print(time.ctime())\n","\n","myNN.train_data()\n","\n","### Test data\n","success = myNN.test_data((myNN.img_w, myNN.img_h))\n","\n","endtime = time.time()\n","print('Time {0}, num_epochs {1}, success {2}'.format((endtime-starttime), myNN.num_epochs, success))\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["20\n","gdrive/My Drive/\n","Mon Apr 15 01:57:51 2019\n","Layer dims...[16384, 25, 13, 5, 1]...Mon Apr 15 01:57:51 2019\n","Data loaded..Mon Apr 15 02:00:00 2019...(16384, 8012)\n","Cost after run 0, iteration 0: 0.760293, Time: 133.601445\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5d2b89de4bb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmyNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m### Test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-681b11842783>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m                   \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m               \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_success\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimization done..'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_success\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-681b11842783>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(self, X, Y, newparams)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Run xval set and compute cost.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mAL_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;31m#                xval_cost, xval_success = compute_cost(AL_xval, Y_xval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mxval_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval_success\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_L2_reg_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL2_lambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_xval' is not defined"]}]},{"metadata":{"id":"wxR1vnITukJn","colab_type":"code","colab":{}},"cell_type":"code","source":["  "],"execution_count":0,"outputs":[]}]}